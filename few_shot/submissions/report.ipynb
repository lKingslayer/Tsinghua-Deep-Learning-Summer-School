{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Description of AlxeNet\n",
    "\n",
    "- `creat()`: This function is to create network graph. The network consists of 8 layers. The first layer uses one `conv()`, one `max_pool()` and one `lrn()`. The second layer also uses one `conv()`, one `max_pool()` and one `lrn()`. The third layer uses one `conv()`. The forth layer uses one `conv()`. The fifth uses one `conv()` and one `max_pool()`. The sixth uses one `tf.reshape()` to flattern, one `fc()` and one `dropout()`. The seventh uses one `fc()` and one `dropout()`. The last layer uses only one `fc()`.\n",
    "\n",
    "- `conv()`: `conv()` uses the mathmetical basic of convolution. A convolutional filter is a set of learnable weights which are learned using the backpropagation algorithm. And Stride denotes how many steps we are moving in each steps in convolution. Padding is to add extra pixels outside the image. And zero padding means every pixel value that you add is zero. \n",
    "\n",
    "- `max_pool()`: This function is for max pooling, which is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n",
    "\n",
    "- `fc()`: ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, itâ€™s described as: $max(0, x)$. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
